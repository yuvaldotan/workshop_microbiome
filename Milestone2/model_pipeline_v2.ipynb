{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'social_model_class' from 'c:\\\\Users\\\\tomer\\\\Desktop\\\\BSc\\\\year3\\\\sem B\\\\workshop_microbiome\\\\Milestone2\\\\social_model_class.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from scipy.spatial.distance import braycurtis\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from imports import *\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# # adding Folder_2 to the system path\n",
    "# sys.path.insert(0, r'C:\\Users\\tomer\\Desktop\\BSc\\year3\\sem B\\workshop_microbiome\\code')\n",
    "# # sys.path.insert(0, r'C:\\Users\\yuvald\\Documents\\Uni\\סמסטר ב\\workshop_microbiome\\code')\n",
    "\n",
    "from imports import  *\n",
    "\n",
    "import social_model_class\n",
    "reload(social_model_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"train_data.csv\"\n",
    "metadata_path = r\"train_metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Delta_t  \\space windowing$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = social_model_class.superModel(data_path, metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation (4 - fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "baboons = model.baboons\n",
    "dict_list = list(model.baboons.items())\n",
    "np.random.shuffle(dict_list)\n",
    "groups = [dict(group) for group in np.array_split(dict_list,4)]\n",
    "\n",
    "data_df = pd.read_csv(data_path, index_col=0)\n",
    "metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "for i in range(4):\n",
    "    train_metatdata = metadata_df[~metadata_df[\"baboon_id\"].isin(groups[i].keys())]\n",
    "    train_data = data_df.loc[train_metatdata.index]\n",
    "    known_metadata = metadata_df[metadata_df['baboon_id'].isin(groups[i].keys())]\n",
    "    indicies = []\n",
    "    for baboon in groups[i].values():\n",
    "        indicies.extend(baboon.metadata_I.index[:10])\n",
    "\n",
    "    known_data = data_df.loc[indicies]\n",
    "    true_data = data_df.loc[known_metadata[~np.isin(known_metadata.index, indicies)].index]\n",
    "\n",
    "\n",
    "    train_data.to_csv(fr\".\\cross_val_data\\train_data_{i}.csv\")\n",
    "    train_metatdata.to_csv(fr\".\\cross_val_data\\train_metadata_{i}.csv\")\n",
    "    known_data.to_csv(fr\".\\cross_val_data\\known_data_{i}.csv\")\n",
    "    known_metadata.to_csv(fr\".\\cross_val_data\\known_metadata_{i}.csv\")\n",
    "    true_data.to_csv(fr\".\\cross_val_data\\true_data_{i}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(data_path, metadata_path):\n",
    "    \"\"\"Preprocess the data and metadata files to be used in.\n",
    "    for samples from the same date and baboon_id, calculate the mean of the bacteria counts.\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    temp = pd.merge(data_df, metadata_df[[\"sample\", \"baboon_id\", \"collection_date\"]], on=\"sample\")\n",
    "    bacteria_columns = temp.columns[1:-2]  # Adjust this depending on your actual column structure\n",
    "\n",
    "    # Group by baboon_id and collection_date\n",
    "    data_clean = temp.groupby(['baboon_id', 'collection_date']).agg({**{col: 'mean' for col in bacteria_columns}, 'sample': 'first'}).reset_index()\n",
    "    data_clean.drop(['baboon_id', 'collection_date'], axis=1, inplace=True)\n",
    "    chosen_samples = data_clean[\"sample\"].unique()\n",
    "\n",
    "    # to drop = data_idx - chosen samples\n",
    "    # metadata_idx.drop (to_drop)\n",
    "    to_drop = data_df[~data_df[\"sample\"].isin(chosen_samples)].index\n",
    "    metadata_clean = metadata_df.drop(to_drop)\n",
    "    metadata_clean.set_index('sample', inplace=True)\n",
    "    data_clean.set_index('sample', inplace=True)\n",
    "    metadata_clean['collection_date'] = pd.to_datetime(metadata_clean['collection_date'])\n",
    "    return metadata_clean, data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    model = social_model_class.superModel(fr\".\\cross_val_data\\train_data_{i}.csv\", fr\".\\cross_val_data\\train_metadata_{i}.csv\")\n",
    "    model.alpha_ = np.array([0]*61)\n",
    "    \n",
    "    cpus = max(1, min(multiprocessing.cpu_count() - 2, len(model.baboons)))\n",
    "    with ProcessPoolExecutor(cpus) as executor:\n",
    "        futures = [executor.submit(baboon.fit, model.alpha_) for baboon in model.baboons.values()]\n",
    "        for future in as_completed(futures):\n",
    "            pass\n",
    "    \n",
    "    print(f\"predicting {i}\")\n",
    "    \n",
    "    model.add_new_data(fr\".\\cross_val_data\\known_data_{i}.csv\", fr\".\\cross_val_data\\known_metadata_{i}.csv\")\n",
    "\n",
    "    print(f\"new data added {i}\")\n",
    "    predictions_noniterative = model.predict(groups[i].keys(), iterative=False)\n",
    "    predictions_iterative = model.predict(groups[i].keys(), iterative=True)\n",
    "    predictions_noniterative.to_csv(fr\".\\cross_val_data\\predictions_noniterative_{i}.csv\")\n",
    "    predictions_iterative.to_csv(fr\".\\cross_val_data\\predictions_iterative_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 15))\n",
    "for i in range(4):\n",
    "    predictions_noniterative = pd.read_csv(fr\".\\cross_val_data\\predictions_noniterative_{i}.csv\", index_col=0)\n",
    "    predictions_iterative = pd.read_csv(fr\".\\cross_val_data\\predictions_iterative_{i}.csv\", index_col=0)\n",
    "    true_data = pd.read_csv(fr\".\\cross_val_data\\true_data_{i}.csv\", index_col=0)\n",
    "\n",
    "    for baboon in groups[i].keys():\n",
    "        bc_noniterative = []\n",
    "        bc_iterative = []\n",
    "        for idx in np.intersect1d(predictions_noniterative.index, groups[i][baboon].metadata_I.index):\n",
    "            score_iter = 0\n",
    "            score_noniter = 0\n",
    "            score_iter += braycurtis(true_data.loc[idx], predictions_iterative.loc[idx])/len(true_data)\n",
    "            score_noniter += braycurtis(true_data.loc[idx], predictions_noniterative.loc[idx])/len(true_data)\n",
    "\n",
    "\n",
    "        bc_noniterative.append(score_noniter)\n",
    "        bc_iterative.append(score_iter)\n",
    "    sns.kdeplot(bc_noniterative, ax=axes[i, 0], label=\"noniterative\", axes=axes[i, 0])\n",
    "    sns.kdeplot(bc_iterative, ax=axes[i, 1], label=\"iterative\", axes=axes[i, 0])\n",
    "\n",
    "axes[0,0].set_title(\"non-iterative\")\n",
    "axes[0,1].set_title(\"iterative\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 15))\n",
    "for i in range(4):\n",
    "    predictions_noniterative = pd.read_csv(fr\".\\cross_val_data\\predictions_noniterative_{i}.csv\", index_col=0)\n",
    "    predictions_iterative = pd.read_csv(fr\".\\cross_val_data\\predictions_iterative_{i}.csv\", index_col=0)\n",
    "    true_data = pd.read_csv(fr\".\\cross_val_data\\true_data_{i}.csv\", index_col=0)\n",
    "\n",
    "    for baboon in groups[i].keys():\n",
    "        bc_noniterative = []\n",
    "        bc_iterative = []\n",
    "        for idx in np.intersect1d(predictions_noniterative.index, groups[i][baboon].metadata_I.index):\n",
    "            score_iter = 0\n",
    "            score_noniter = 0\n",
    "            score_iter += braycurtis(true_data.loc[idx], predictions_iterative.loc[idx])/len(true_data)\n",
    "            score_noniter += braycurtis(true_data.loc[idx], predictions_noniterative.loc[idx])/len(true_data)\n",
    "\n",
    "\n",
    "        bc_noniterative.append(score_noniter)\n",
    "        bc_iterative.append(score_iter)\n",
    "    sns.kdeplot(bc_noniterative, ax=axes[0, 0], label=\"noniterative\", axes=axes[i, 0], label=f\"fold {i}\")\n",
    "    sns.kdeplot(bc_iterative, ax=axes[0, 1], label=\"iterative\", axes=axes[i, 0], label=f\"fold {i}\")\n",
    "\n",
    "axes[0,0].set_title(\"non-iterative\")\n",
    "axes[0,1].set_title(\"iterative\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.baboons[0].fit(np.array([1/3]*61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(metadata_path)\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "metadata_df[\"collection_date\"] = pd.to_datetime(metadata_df[\"collection_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.set_index('sample', inplace=True)\n",
    "data_df.set_index('sample', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = social_model_class.superModel(data_path,   metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"collection_date_datetime\"] = pd.to_datetime(metadata_df[\"collection_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.drop_duplicates(subset=[\"baboon_id\", \"collection_date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_samples_df = metadata_df[[\"social_group\", \"collection_date_datetime\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_samples_df[\"delta_t_cnt\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in time_samples_df[\"social_group\"].unique():\n",
    "    for t in time_samples_df[\"collection_date_datetime\"][time_samples_df[\"social_group\"] == group]:\n",
    "        count = len(time_samples_df[(time_samples_df[\"social_group\"]==group) & (abs(time_samples_df[\"collection_date_datetime\"]-t).dt.days<=10)])\n",
    "        time_samples_df[\"delta_t_cnt\"][(time_samples_df[\"social_group\"]==group) & (time_samples_df[\"collection_date_datetime\"]==t)] = count\n",
    "time_samples_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in time_samples_df[\"social_group\"].unique():\n",
    "    plt.plot(sorted(time_samples_df[\"collection_date_datetime\"][time_samples_df[\"social_group\"]==group]), time_samples_df[\"delta_t_cnt\"][time_samples_df[\"social_group\"]==group], label=group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in time_samples_df.index:\n",
    "    delta_t_datetimes = [i for i in range(t-7,t+7)]\n",
    "    time_samples_df[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_samples_df.loc[pd.to_datetime('2000-05-19')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
